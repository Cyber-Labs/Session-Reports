# Session of Machine Learning Division of CyberLabs
Conducted on: 03/03/2025

## Agenda
Discussion of DeepLearning Specialization Course 2.

## Summary
1.  Relation of Batch Normalization with vanishing and exploding gradients.
2.  Batch Normalization while test time.
3.  Keep Probability in Dropout Regularization.
4.  Gradient Checking and Dropout.
5.  Momentum and RMSProp (Necessity and Difference).
6.  Nesterov Momentum.
7.  SGD Momentum.
8.  Bayes Error.
9.  ADAM Optimization Algorithm.
10. Batch Normalization on A or on Z.
11. Bias Correction in ADAM but not in Momentum and RMSProp why?

## Agenda for the next session
* DeepLearning Course 4 (Convolutional Neural Networks)

## Report Compiled by
Ritesh Kumbhare

## Attendees
Second Year Attendees: Green Kedia, Mukil M, Harshvardhan Saini, Daksh Mor

First Year Attendees: Anab, Arnav, Ritesh, Rajat, Arjav, Abhishek, Ayushman, Sreenandan, Anukul

## Absentees
First Year: None
