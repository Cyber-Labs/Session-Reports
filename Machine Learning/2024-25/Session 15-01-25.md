# Session of Machine Learning Division of CyberLabs  
**Conducted on:** 15 January 2025  
**Time:** 11 PM  
**Mode:** Offline  

## Agenda  
1. Discussed character-level and word-level tokenization.  
2. **Byte Pair Encoding (BPE):** A subword tokenization method that merges the most frequent pairs of characters or subwords iteratively to create a compact and efficient vocabulary for text representation.  
3. Discussed why BPE is better than character-level and word-level tokenization.  
4. Also discussed a few different tokenization techniques like One-Hot Encoding, Bag of Words, and N-grams.  

## Agenda for the next session  
**Discussion on Attention mechanism**  

## Next Session Details  
**Date:** 17 January 2025  

## Credits  
**Report Compiled by:** Priyam Pritam Panda  

## Attendees  
**Second Year Attendees:** All present  

**Pre-final Year Attendees:** Samyak Jha, Manav Jain, Anany Garg, Ganesh Talwar  
