# Session of Machine Learning Division of CyberLabs  
**Conducted on:** 24 December 2024  
**Time:** 12 PM  
**Mode:** Online  

## Agenda  
1. Different types of Normalizations:  
    a) **Batch Normalization:** Normalizes inputs across the batch dimension to stabilize and accelerate training.  
    b) **Layer Normalization:** Normalizes inputs across features for each input sample, useful in sequential data.  
    c) **Group Normalization:** Divides channels into groups and normalizes them independently to handle small batch sizes effectively.  
    d) **RMS Normalization:** Normalizes weights using their root mean square, often applied in transformer architectures.  

2. Advantages and disadvantages of each of the norms.  
3. When to use which normalization.  

## Agenda for the next session  
**Discussion on Byte Pair Encoding (BPE)**  

## Next Session Details  
**Date:** 15 January 2025  

## Credits  
**Report Compiled by:** Priyam Pritam Panda  

## Attendees  
**Second Year Attendees:** All present  

**Pre-final Year Attendees:** Samyak Jha, Manav Jain, Karaka Prasanth Naidu  
