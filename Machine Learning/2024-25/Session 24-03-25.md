# Session of Machine Learning Division of CyberLabs:

Conducted on 24th March 2025

## Agenda of Meet

Paper discussion of Llama, Mistral and Flash Attention and topics related to them.

## Important Topics Discussed:

- Detailed discussion on the architecture and innovations in Mistral and LLaMA models
- Analysis of Mistral’s improvements over LLaMA in efficiency and scaling
- Discussion on key design decisions in decoder-only architectures
- **RoPE (Rotary Positional Encoding):** Understanding its role in enhancing transformer models’ positional representations.
- **MQA (Multi-Query Attention):** Exploration of its efficiency benefits in decoder-only models
- **KV Cache (Key-Value Caching):** Discussion on optimizing inference time using KV cache techniques
- **SWA (Sliding Window Attention):** Review of how SWA enables long-context processing
- **Flash Attention:** Discussion of how GPUs work and how Flash attention leverages this for faster implementation of attention.

## Report Compiled by:

Mukil M

## Attendees:

Pre Final Years: Manav Jain, Samyak Jha, Prasanth Naidu, Ganesh Talwar

Second Years: Green Kedia, Abhinav Jha, Daksh Mor, Dilshad Raza, Mohd. Ashaz Khan, Mukil M, Harshvardhan Saini, Geeth Chand Chinni

Absentees: Jeevesh Kumar, Priyam Pritam Panda
