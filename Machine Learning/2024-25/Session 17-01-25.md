# Session of Machine Learning Division of CyberLabs  
**Conducted on:** 17 January 2025  
**Time:** 7 PM  
**Mode:** Offline  

## Agenda  
1. **"Attention is All You Need":** Discussed the research paper introducing the Transformer model, which relies solely on self-attention mechanisms to process sequences.  
2. Discussed the use of multi-head attention, positional encodings, and layer normalization, eliminating the need for recurrence or convolution.  
3. Discussed the architecture of the encoder and decoder and their working.  

## Agenda for the next session  
**Implementation of Attention Mechanism**  

## Next Session Details  
**Date:** 21 January 2025  

## Credits  
**Report Compiled by:** Priyam Pritam Panda  

## Attendees  
**Second Year Attendees:** All present  

**Pre-final Year Attendees:** Samyak Jha, Manav Jain, Karaka Prasanth Naidu  
