# Session 2

12-hour long Kaggle competition marathon. Members were divided into groups of 3 to work on the competition "Titanic Survival Prediction".

## Brief Summary

Concepts learned:

- **Tree-based Models**: Gradient Boosting Decision Tree (GBDT), Random Forest
- **Ensembling** : Bagging, boosting, and stacking
  `XGBoost`  for ensembling.
- **One-Hot Encoding** : 
  - Dummy variable trap
  - `pd.get_dummies()` function
  - `drop_first` parameter
- **Functions**: `enumerate`, `df.loc` , `df.iloc`
- **Future Work**:
  - Parameter tuning for the chosen classifier 
  - Plotting loss curve.
  - Splitting the data in train set and validation set.

## Credits

*Conducted by:* [Suyog Jadhav](https://github.com/IAmSuyogJadhav) and [Udbhav Bamba](https://github.com/ubamba98) on 27-01-2019

*Report compiled by*: [Nainesh Hulke](https://github.com/naineshhulke)